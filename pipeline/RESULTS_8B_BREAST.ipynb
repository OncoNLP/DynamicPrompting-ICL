{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1,2,3,4'\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import string\n",
    "import argparse\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import softmax\n",
    "from collections import OrderedDict\n",
    "from vllm import LLM, SamplingParams\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, AutoConfig\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 101)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_pickle('/data/datasets/outcome/breast/breast_test_180_1.pkl')\n",
    "y_test = list(df_test['label'])\n",
    "del y_test[37]\n",
    "\n",
    "df_train = pd.read_pickle('/data/datasets/outcome/breast/breast_train_180_1.pkl')\n",
    "y_train = list(df_train['label'])\n",
    "del y_train[62]\n",
    "len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, 400, 101)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_templates = pd.read_pickle('/data/llm_shared/breast_summaries/templates_train.pkl')\n",
    "test_templates = pd.read_csv('/data/llm_shared/breast_summaries/templates_test.csv', header=None)\n",
    "test_templates = list(test_templates[1][1:])\n",
    "\n",
    "del test_templates[37]\n",
    "del train_templates[62]\n",
    "templates = train_templates + test_templates\n",
    "len(templates), len(train_templates), len(test_templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folds\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((train_templates, test_templates))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "strat = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_tr = []\n",
    "idxs_t = []\n",
    "for fold, (train_idx, test_idx) in enumerate(strat.split(X, y), 1):\n",
    "    X_train_fold, X_test_fold = X[train_idx], X[test_idx]\n",
    "    y_train_fold, y_test_fold = y[train_idx], y[test_idx]\n",
    "    idxs_tr.append(train_idx)\n",
    "    idxs_t.append(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 400)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold = 1\n",
    "with open('/data/llm_shared/jina_embs/jina_breast_templates.pkl', 'rb') as f:\n",
    "   train_embeddings = pickle.load(f)\n",
    "fold = fold - 1\n",
    "notes = X\n",
    "labels = y\n",
    "ids = idxs_t[fold]\n",
    "notes_test = [notes[i] for i in ids]\n",
    "y_test = [labels[i] for i in ids]\n",
    "embs_test = [train_embeddings[i] for i in ids]\n",
    "ids = idxs_tr[fold]\n",
    "notes_train = [notes[i] for i in ids]\n",
    "y_train = [labels[i] for i in ids]\n",
    "embs_train = [train_embeddings[i] for i in ids]\n",
    "\n",
    "len(embs_test), len(embs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the pipeline - 8B Experiments\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 5\n",
    "fold = fold - 1\n",
    "ids = idxs_t[fold]\n",
    "notes_test = [notes[i] for i in ids]\n",
    "y_test = [labels[i] for i in ids]\n",
    "embs_test = [train_embeddings[i] for i in ids]\n",
    "ids = idxs_tr[fold]\n",
    "notes_train = [notes[i] for i in ids]\n",
    "y_train = [labels[i] for i in ids]\n",
    "embs_train = [train_embeddings[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_train['note'] = notes_train\n",
    "df_train['label'] = y_train\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "df_test['note'] = notes_test\n",
    "df_test['label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"../../example_file_5.csv\")\n",
    "df_test.to_csv(\"../../test_file_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting script execution...\n",
      "\n",
      "Processing data...\n",
      "Loaded 401 examples and 100 test samples.\n",
      "\n",
      "Loading embedding model...\n",
      "\n",
      "Creating vector database...\n",
      "Generating embeddings: 100%|██████████████████| 401/401 [00:34<00:00, 11.72it/s]\n",
      "Vector database created successfully.\n",
      "\n",
      "Initializing LLM model...\n",
      "INFO 02-25 16:45:48 config.py:890] Defaulting to use mp for distributed inference\n",
      "WARNING 02-25 16:45:48 arg_utils.py:862] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 02-25 16:45:48 config.py:999] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 02-25 16:45:48 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='gradientai/Llama-3-8B-Instruct-262k', speculative_config=None, tokenizer='gradientai/Llama-3-8B-Instruct-262k', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=gradientai/Llama-3-8B-Instruct-262k, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "WARNING 02-25 16:45:49 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 48 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 02-25 16:45:49 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:45:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m INFO 02-25 16:45:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m INFO 02-25 16:45:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 02-25 16:45:51 utils.py:977] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:45:51 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 02-25 16:45:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m INFO 02-25 16:45:51 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 02-25 16:45:51 utils.py:977] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:45:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m INFO 02-25 16:45:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 02-25 16:45:51 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m WARNING 02-25 16:45:52 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 02-25 16:45:52 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m WARNING 02-25 16:45:52 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m WARNING 02-25 16:45:52 custom_all_reduce.py:122] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 02-25 16:45:52 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7ff6f00cf0d0>, local_subscribe_port=44463, remote_subscribe_port=None)\n",
      "INFO 02-25 16:45:52 model_runner.py:915] Starting to load model gradientai/Llama-3-8B-Instruct-262k...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m INFO 02-25 16:45:52 model_runner.py:915] Starting to load model gradientai/Llama-3-8B-Instruct-262k...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:45:52 model_runner.py:915] Starting to load model gradientai/Llama-3-8B-Instruct-262k...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m INFO 02-25 16:45:52 model_runner.py:915] Starting to load model gradientai/Llama-3-8B-Instruct-262k...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m INFO 02-25 16:45:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:45:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m INFO 02-25 16:45:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "INFO 02-25 16:45:52 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  4.47it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  2.97it/s]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m INFO 02-25 16:45:54 model_runner.py:926] Loading model weights took 3.8023 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:45:55 model_runner.py:926] Loading model weights took 3.8023 GB\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.50it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.76it/s]\n",
      "\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m INFO 02-25 16:45:55 model_runner.py:926] Loading model weights took 3.8023 GB\n",
      "INFO 02-25 16:45:55 model_runner.py:926] Loading model weights took 3.8023 GB\n",
      "INFO 02-25 16:45:56 distributed_gpu_executor.py:57] # GPU blocks: 61284, # CPU blocks: 8192\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:45:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:45:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 02-25 16:45:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 02-25 16:45:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m INFO 02-25 16:45:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m INFO 02-25 16:45:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m INFO 02-25 16:45:59 model_runner.py:1217] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m INFO 02-25 16:45:59 model_runner.py:1221] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652868)\u001b[0;0m INFO 02-25 16:46:20 model_runner.py:1335] Graph capturing finished in 21 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652873)\u001b[0;0m INFO 02-25 16:46:20 model_runner.py:1335] Graph capturing finished in 21 secs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1652869)\u001b[0;0m INFO 02-25 16:46:20 model_runner.py:1335] Graph capturing finished in 21 secs.\n",
      "INFO 02-25 16:46:20 model_runner.py:1335] Graph capturing finished in 21 secs.\n",
      "Loaded LLM model: gradientai/Llama-3-8B-Instruct-262k\n",
      "\n",
      "Starting model evaluation...\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  5.73it/s, est. speed input: 2049.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 12.16it/s, est. speed input: 4342.9\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 17.06it/s, est. speed input: 6270.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 14.54it/s, est. speed input: 6567.1\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.07it/s, est. speed input: 3329.9\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.37it/s, est. speed input: 3935.9\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.06it/s, est. speed input: 3123.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.89it/s, est. speed input: 3971.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.03it/s, est. speed input: 3050.6\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.32it/s, est. speed input: 4036.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.11it/s, est. speed input: 3200.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.17it/s, est. speed input: 4469.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.41it/s, est. speed input: 3260.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 12.48it/s, est. speed input: 4220.1\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.43it/s, est. speed input: 3401.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.91it/s, est. speed input: 3685.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.36it/s, est. speed input: 3213.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.04it/s, est. speed input: 4043.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.16it/s, est. speed input: 3282.3\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.80it/s, est. speed input: 4256.6\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.02it/s, est. speed input: 3093.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 12.17it/s, est. speed input: 4675.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.69it/s, est. speed input: 3301.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 13.84it/s, est. speed input: 5469.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 15.23it/s, est. speed input: 7441.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 13.30it/s, est. speed input: 5097.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.73it/s, est. speed input: 3124.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.37it/s, est. speed input: 4253.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.67it/s, est. speed input: 3250.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.57it/s, est. speed input: 4082.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.95it/s, est. speed input: 3208.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.65it/s, est. speed input: 4325.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.46it/s, est. speed input: 3100.6\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 12.91it/s, est. speed input: 4390.9\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.54it/s, est. speed input: 3551.9\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.69it/s, est. speed input: 4597.1\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.13it/s, est. speed input: 3245.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.85it/s, est. speed input: 4174.3\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.01it/s, est. speed input: 3248.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.47it/s, est. speed input: 4335.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.19it/s, est. speed input: 3272.1\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 12.27it/s, est. speed input: 4752.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.98it/s, est. speed input: 3217.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.00it/s, est. speed input: 4053.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.45it/s, est. speed input: 3252.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.55it/s, est. speed input: 4879.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.14it/s, est. speed input: 3249.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 13.22it/s, est. speed input: 5225.3\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 15.91it/s, est. speed input: 6988.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 13.65it/s, est. speed input: 5202.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.29it/s, est. speed input: 3261.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 12.11it/s, est. speed input: 4739.9\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.99it/s, est. speed input: 3131.6\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.91it/s, est. speed input: 4261.3\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.44it/s, est. speed input: 3102.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.03it/s, est. speed input: 4482.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.76it/s, est. speed input: 3243.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.94it/s, est. speed input: 4359.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.71it/s, est. speed input: 3126.6\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.21it/s, est. speed input: 4197.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.19it/s, est. speed input: 3339.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.68it/s, est. speed input: 3839.6\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.68it/s, est. speed input: 3139.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.96it/s, est. speed input: 3959.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.24it/s, est. speed input: 3164.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.98it/s, est. speed input: 4410.3\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.98it/s, est. speed input: 3479.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.97it/s, est. speed input: 4728.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.58it/s, est. speed input: 3423.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.87it/s, est. speed input: 3970.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.15it/s, est. speed input: 3262.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 13.22it/s, est. speed input: 5146.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 16.47it/s, est. speed input: 6516.6\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 15.63it/s, est. speed input: 6291.3\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.66it/s, est. speed input: 3563.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.50it/s, est. speed input: 3828.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.08it/s, est. speed input: 3213.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.56it/s, est. speed input: 4882.1\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.02it/s, est. speed input: 3355.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.92it/s, est. speed input: 4223.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.69it/s, est. speed input: 3297.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 12.07it/s, est. speed input: 4638.2\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.72it/s, est. speed input: 3109.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.47it/s, est. speed input: 4283.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.45it/s, est. speed input: 3236.3\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.83it/s, est. speed input: 3660.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.75it/s, est. speed input: 3439.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 11.20it/s, est. speed input: 4606.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.32it/s, est. speed input: 3215.5\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 12.59it/s, est. speed input: 4472.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.01it/s, est. speed input: 3066.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  9.29it/s, est. speed input: 3798.1\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  7.86it/s, est. speed input: 3061.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 13.75it/s, est. speed input: 5204.4\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 16.30it/s, est. speed input: 6850.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 13.71it/s, est. speed input: 5104.0\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.11it/s, est. speed input: 3339.7\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.71it/s, est. speed input: 4521.8\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00,  8.75it/s, est. speed input: 3225.1\n",
      "Processed prompts: 100%|█| 1/1 [00:00<00:00, 10.66it/s, est. speed input: 4075.3\n",
      "Computing evaluation metrics...\n",
      "Evaluation complete. Metrics:\n",
      "  - Accuracy: 0.8100\n",
      "  - Precision: 0.8021\n",
      "  - Recall: 1.0000\n",
      "  - F1 Score: 0.8902\n",
      "  - AUC: 0.7388\n",
      "\n",
      "Metrics saved to evaluation_metrics_zero_shot_summary.txt\n",
      "Script execution finished successfully.\n",
      "ERROR 02-25 16:46:33 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 1652869 died, exit code: -15\n",
      "INFO 02-25 16:46:33 multiproc_worker_utils.py:123] Killing local vLLM worker processes\n"
     ]
    }
   ],
   "source": [
    "!/home/srinivasb/.conda/envs/radoncB/bin/python3 run_dynamic_prompt_B.py \\\n",
    "    --exp_type zero_shot_summary \\\n",
    "    --large False \\\n",
    "    --num_gpus 4 \\\n",
    "    --zero_shot True \\\n",
    "    --examples ../../example_file_3.csv \\\n",
    "    --test_data ../../test_file_3.csv \\\n",
    "    --summary True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
